{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3063bdf6-0b9c-4e6c-8c92-da17797d71b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "import os\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import re\n",
    "from datetime import datetime\n",
    "import csv\n",
    "import os\n",
    "\n",
    "\n",
    "# Utility Functions\n",
    "def get_timestamp():\n",
    "    \"\"\"Get the current timestamp.\"\"\"\n",
    "    return datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "\n",
    "def create_output_folder(model_name):\n",
    "    \"\"\"\n",
    "    Creates a timestamped output folder for a given model name.\n",
    "    Replaces forbidden characters in the folder name.\n",
    "    \"\"\"\n",
    "    base_output_path = os.path.join(\"voiceapp\", \"output\")\n",
    "    summary_folder_path = os.path.join(base_output_path, \"summaries_folder\")\n",
    "    \n",
    "    os.makedirs(base_output_path, exist_ok=True)\n",
    "    os.makedirs(summary_folder_path, exist_ok=True)\n",
    "\n",
    "    # Sanitize the model name by replacing forbidden characters\n",
    "    safe_model_name = re.sub(r'[<>:\"/\\\\|?*]', '_', model_name)\n",
    "\n",
    "    timestamp = get_timestamp()\n",
    "    folder_name = f\"{safe_model_name}_{timestamp}\"\n",
    "\n",
    "    model_folder_path = os.path.join(summary_folder_path, folder_name)\n",
    "    os.makedirs(model_folder_path, exist_ok=True)\n",
    "\n",
    "    return summary_folder_path, model_folder_path\n",
    "\n",
    "\n",
    "def compute_similarity(text1, text2, tokenizer, model):\n",
    "    \"\"\"Compute cosine similarity between two text embeddings.\"\"\"\n",
    "    inputs1 = tokenizer(text1, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "    inputs2 = tokenizer(text2, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        emb1 = model(**inputs1).last_hidden_state.mean(dim=1)\n",
    "        emb2 = model(**inputs2).last_hidden_state.mean(dim=1)\n",
    "\n",
    "    cosine_sim = torch.nn.functional.cosine_similarity(emb1, emb2)\n",
    "    return cosine_sim.item()\n",
    "\n",
    "\n",
    "def load_file_paths(file_path):\n",
    "    \"\"\"Load and clean file paths from a given list.\"\"\"\n",
    "    with open(file_path, 'r') as f:\n",
    "        return [line.strip() for line in f.readlines()]\n",
    "\n",
    "\n",
    "def chunk_text(text, chunk_size=1000):\n",
    "    \"\"\"Split text into smaller chunks to fit within LLM token limits.\"\"\"\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    for i in range(0, len(words), chunk_size):\n",
    "        chunk = ' '.join(words[i:i + chunk_size])\n",
    "        chunks.append(chunk)\n",
    "    return chunks\n",
    "\n",
    "def generate_summaries_for_chunk(chunk, model_name, tokenizer, embed_model, output_folder):\n",
    "    \"\"\"Generate summary for a single chunk of text.\"\"\"\n",
    "    response = ollama.chat(\n",
    "        model=model_name,\n",
    "        messages=[{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"\"\"\n",
    "                ---\n",
    "                **\"You are a copywriter. Your task is to summarize the provided text according to the following framework:**\n",
    "                1. **Key Themes**: Identify and explain the main themes or topics discussed in the text.\n",
    "                2. **Impacts**: Assess the broader impacts, highlighting economic, technological, political, and social dimensions.\n",
    "                3. **Examples and Evidence**: Draw connections to real-world examples or supporting evidence that underline the key points.\n",
    "                4. **Opportunities and Risks**: Explore potential opportunities and risks suggested by the text.\n",
    "                5. **Conclusion**: Summarize the implications and suggest future considerations or actions that align with the insights presented in the text.\n",
    "                ---\n",
    "            \"{chunk}\" \"\"\"\n",
    "        }]\n",
    "    )\n",
    "    return extract_summary(response)\n",
    "\n",
    "def create_csv_file(csv_path):\n",
    "    \"\"\"Creates a CSV file if it does not exist and writes headers.\"\"\"\n",
    "    if not os.path.exists(csv_path):\n",
    "        with open(csv_path, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "            fieldnames = ['summary_path', 'reference_path']\n",
    "            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "            writer.writeheader()\n",
    "\n",
    "\n",
    "def is_record_in_csv(csv_path, summary_path, reference_path):\n",
    "    \"\"\"Checks if the record (summary_path, reference_path) already exists in the CSV.\"\"\"\n",
    "    if not os.path.exists(csv_path):\n",
    "        return False\n",
    "    \n",
    "    with open(csv_path, 'r', newline='', encoding='utf-8') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            if row['summary_path'] == summary_path and row['reference_path'] == reference_path:\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def save_to_csv(csv_path, summary_path, reference_path):\n",
    "    \"\"\"Appends a record to the CSV file if it does not exist.\"\"\"\n",
    "    if not is_record_in_csv(csv_path, summary_path, reference_path):\n",
    "        with open(csv_path, 'a', newline='', encoding='utf-8') as csvfile:\n",
    "            fieldnames = ['summary_path', 'reference_path']\n",
    "            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "            writer.writerow({'summary_path': summary_path, 'reference_path': reference_path})\n",
    "    else:\n",
    "        print(f\"Record already exists: {summary_path} -> {reference_path}\")\n",
    "\n",
    "def extract_summary(response):\n",
    "    \"\"\"Extract summary content from the model response.\"\"\"\n",
    "    if isinstance(response, list):\n",
    "        for message in response:\n",
    "            if 'content' in message:\n",
    "                return message['content']\n",
    "    elif isinstance(response, dict):\n",
    "        return response.get('message', {}).get('content', \"No content available\")\n",
    "    else:\n",
    "        return \"Error summarizing file\"\n",
    "\n",
    "def generate_summaries(file_path, model_name, tokenizer, embed_model, summary_folder, iterations=5, csv_path=None):\n",
    "    \"\"\"Generates summaries for a single file, saves each iteration and appends paths to CSV if not exist.\"\"\"\n",
    "    summaries = []\n",
    "    similarity_scores = []\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8-sig') as f:\n",
    "        text = f.read()\n",
    "\n",
    "    # Chunk the text\n",
    "    chunks = chunk_text(text)\n",
    "\n",
    "    base_filename = os.path.basename(file_path).split('.txt')[0]\n",
    "\n",
    "    for chunk in chunks:\n",
    "        for i in range(1, iterations + 1):\n",
    "            summary = generate_summaries_for_chunk(chunk, model_name, tokenizer, embed_model, summary_folder)\n",
    "            summaries.append(summary)\n",
    "\n",
    "            # Compute similarity\n",
    "            similarity = compute_similarity(text, summary, tokenizer, embed_model)\n",
    "            similarity_scores.append(similarity)\n",
    "\n",
    "            summary_file_path = os.path.join(summary_folder, f\"{base_filename}-summary_{i}.txt\")\n",
    "            try:\n",
    "                with open(summary_file_path, 'w', encoding='utf-8') as f:\n",
    "                    f.write(summary)\n",
    "\n",
    "                # Save to CSV\n",
    "                if csv_path:\n",
    "                    save_to_csv(csv_path, summary_file_path, file_path)\n",
    "\n",
    "            except UnicodeEncodeError as e:\n",
    "                print(f\"Encoding error when writing summary {i} for file {file_path}: {e}\")\n",
    "\n",
    "    return summaries\n",
    "\n",
    "\n",
    "def process_files_for_model(model_name, file_paths, tokenizer, embed_model):\n",
    "    \"\"\"Processes all files for a given model and saves results to CSV.\"\"\"\n",
    "    # Create both the summaries folder and model folder with timestamp\n",
    "    summary_folder, model_folder = create_output_folder(model_name)\n",
    "\n",
    "    # Define the CSV path in the summaries folder (one level back)\n",
    "    csv_path = os.path.join(summary_folder, 'summaries_list.csv')\n",
    "\n",
    "    create_csv_file(csv_path)\n",
    "\n",
    "    pbar = tqdm(total=len(file_paths), desc=f\"Summarizing files for {model_name}\")\n",
    "\n",
    "    for file_path in file_paths:\n",
    "        file_path = file_path.replace('.mp3', '.txt').lstrip()\n",
    "\n",
    "        if os.path.exists(file_path):\n",
    "            start_time = time.time()\n",
    "            summaries = generate_summaries(file_path, model_name, tokenizer, embed_model, model_folder, csv_path=csv_path)\n",
    "\n",
    "            pbar.update(1)\n",
    "            pbar.set_postfix({\n",
    "                'file': os.path.basename(file_path),\n",
    "                'summaries_count': len(summaries),\n",
    "                'time': f\"{time.time() - start_time:.2f} seconds\"\n",
    "            })\n",
    "\n",
    "    pbar.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7e04ccb4-67d4-43d9-9d77-2e79d38b84f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Summarizing files for mistral:7b:   0%|                                                                                                                                                                    | 0/6 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Record already exists: voiceapp\\output\\summaries_folder\\mistral_7b_20241208_213239\\CJG_01_2023_01_14-summary_1.txt -> C:/Users/G/Documents/GitHub/audycje.com.pl/content/audio\\CJG_01_2023_01_14.txt\n",
      "Record already exists: voiceapp\\output\\summaries_folder\\mistral_7b_20241208_213239\\CJG_01_2023_01_14-summary_2.txt -> C:/Users/G/Documents/GitHub/audycje.com.pl/content/audio\\CJG_01_2023_01_14.txt\n",
      "Record already exists: voiceapp\\output\\summaries_folder\\mistral_7b_20241208_213239\\CJG_01_2023_01_14-summary_3.txt -> C:/Users/G/Documents/GitHub/audycje.com.pl/content/audio\\CJG_01_2023_01_14.txt\n",
      "Record already exists: voiceapp\\output\\summaries_folder\\mistral_7b_20241208_213239\\CJG_01_2023_01_14-summary_4.txt -> C:/Users/G/Documents/GitHub/audycje.com.pl/content/audio\\CJG_01_2023_01_14.txt\n",
      "Record already exists: voiceapp\\output\\summaries_folder\\mistral_7b_20241208_213239\\CJG_01_2023_01_14-summary_5.txt -> C:/Users/G/Documents/GitHub/audycje.com.pl/content/audio\\CJG_01_2023_01_14.txt\n",
      "Record already exists: voiceapp\\output\\summaries_folder\\mistral_7b_20241208_213239\\CJG_01_2023_01_14-summary_1.txt -> C:/Users/G/Documents/GitHub/audycje.com.pl/content/audio\\CJG_01_2023_01_14.txt\n",
      "Record already exists: voiceapp\\output\\summaries_folder\\mistral_7b_20241208_213239\\CJG_01_2023_01_14-summary_2.txt -> C:/Users/G/Documents/GitHub/audycje.com.pl/content/audio\\CJG_01_2023_01_14.txt\n",
      "Record already exists: voiceapp\\output\\summaries_folder\\mistral_7b_20241208_213239\\CJG_01_2023_01_14-summary_3.txt -> C:/Users/G/Documents/GitHub/audycje.com.pl/content/audio\\CJG_01_2023_01_14.txt\n",
      "Record already exists: voiceapp\\output\\summaries_folder\\mistral_7b_20241208_213239\\CJG_01_2023_01_14-summary_4.txt -> C:/Users/G/Documents/GitHub/audycje.com.pl/content/audio\\CJG_01_2023_01_14.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Summarizing files for mistral:7b:  17%|██████████████████████████                                                                                                                                  | 1/6 [00:54<04:32, 54.41s/it]\u001b[A\n",
      "Summarizing files for mistral:7b:  17%|██████████████▋                                                                         | 1/6 [00:54<04:32, 54.41s/it, file=CJG_01_2023_01_14.txt, summaries_count=15, time=54.41 seconds]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Record already exists: voiceapp\\output\\summaries_folder\\mistral_7b_20241208_213239\\CJG_01_2023_01_14-summary_5.txt -> C:/Users/G/Documents/GitHub/audycje.com.pl/content/audio\\CJG_01_2023_01_14.txt\n",
      "Record already exists: voiceapp\\output\\summaries_folder\\mistral_7b_20241208_213239\\CJG_02_2023_01_21-summary_1.txt -> C:/Users/G/Documents/GitHub/audycje.com.pl/content/audio\\CJG_02_2023_01_21.txt\n",
      "Record already exists: voiceapp\\output\\summaries_folder\\mistral_7b_20241208_213239\\CJG_02_2023_01_21-summary_2.txt -> C:/Users/G/Documents/GitHub/audycje.com.pl/content/audio\\CJG_02_2023_01_21.txt\n",
      "Record already exists: voiceapp\\output\\summaries_folder\\mistral_7b_20241208_213239\\CJG_02_2023_01_21-summary_3.txt -> C:/Users/G/Documents/GitHub/audycje.com.pl/content/audio\\CJG_02_2023_01_21.txt\n",
      "Record already exists: voiceapp\\output\\summaries_folder\\mistral_7b_20241208_213239\\CJG_02_2023_01_21-summary_4.txt -> C:/Users/G/Documents/GitHub/audycje.com.pl/content/audio\\CJG_02_2023_01_21.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Summarizing files for mistral:7b:  33%|█████████████████████████████▎                                                          | 2/6 [01:35<03:06, 46.72s/it, file=CJG_01_2023_01_14.txt, summaries_count=15, time=54.41 seconds]\u001b[A\n",
      "Summarizing files for mistral:7b:  33%|█████████████████████████████▎                                                          | 2/6 [01:35<03:06, 46.72s/it, file=CJG_02_2023_01_21.txt, summaries_count=10, time=41.33 seconds]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Record already exists: voiceapp\\output\\summaries_folder\\mistral_7b_20241208_213239\\CJG_02_2023_01_21-summary_5.txt -> C:/Users/G/Documents/GitHub/audycje.com.pl/content/audio\\CJG_02_2023_01_21.txt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 21\u001b[0m\n\u001b[0;32m     17\u001b[0m         process_files_for_model(model_name, file_paths, tokenizer, embed_model)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 21\u001b[0m     main()\n",
      "Cell \u001b[1;32mIn[12], line 17\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     14\u001b[0m file_paths \u001b[38;5;241m=\u001b[39m load_file_paths(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvoiceapp\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mlista.txt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model_name \u001b[38;5;129;01min\u001b[39;00m models:\n\u001b[1;32m---> 17\u001b[0m     process_files_for_model(model_name, file_paths, tokenizer, embed_model)\n",
      "Cell \u001b[1;32mIn[10], line 187\u001b[0m, in \u001b[0;36mprocess_files_for_model\u001b[1;34m(model_name, file_paths, tokenizer, embed_model)\u001b[0m\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(file_path):\n\u001b[0;32m    186\u001b[0m     start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m--> 187\u001b[0m     summaries \u001b[38;5;241m=\u001b[39m generate_summaries(file_path, model_name, tokenizer, embed_model, model_folder, csv_path\u001b[38;5;241m=\u001b[39mcsv_path)\n\u001b[0;32m    189\u001b[0m     pbar\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    190\u001b[0m     pbar\u001b[38;5;241m.\u001b[39mset_postfix({\n\u001b[0;32m    191\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfile\u001b[39m\u001b[38;5;124m'\u001b[39m: os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(file_path),\n\u001b[0;32m    192\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msummaries_count\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mlen\u001b[39m(summaries),\n\u001b[0;32m    193\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mstart_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    194\u001b[0m     })\n",
      "Cell \u001b[1;32mIn[10], line 148\u001b[0m, in \u001b[0;36mgenerate_summaries\u001b[1;34m(file_path, model_name, tokenizer, embed_model, summary_folder, iterations, csv_path)\u001b[0m\n\u001b[0;32m    146\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m chunks:\n\u001b[0;32m    147\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, iterations \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m--> 148\u001b[0m         summary \u001b[38;5;241m=\u001b[39m generate_summaries_for_chunk(chunk, model_name, tokenizer, embed_model, summary_folder)\n\u001b[0;32m    149\u001b[0m         summaries\u001b[38;5;241m.\u001b[39mappend(summary)\n\u001b[0;32m    151\u001b[0m         \u001b[38;5;66;03m# Compute similarity\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[10], line 72\u001b[0m, in \u001b[0;36mgenerate_summaries_for_chunk\u001b[1;34m(chunk, model_name, tokenizer, embed_model, output_folder)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_summaries_for_chunk\u001b[39m(chunk, model_name, tokenizer, embed_model, output_folder):\n\u001b[0;32m     71\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Generate summary for a single chunk of text.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 72\u001b[0m     response \u001b[38;5;241m=\u001b[39m ollama\u001b[38;5;241m.\u001b[39mchat(\n\u001b[0;32m     73\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel_name,\n\u001b[0;32m     74\u001b[0m         messages\u001b[38;5;241m=\u001b[39m[{\n\u001b[0;32m     75\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     76\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;124m                ---\u001b[39m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;124m                **\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are a copywriter. Your task is to summarize the provided text according to the following framework:**\u001b[39m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;124m                1. **Key Themes**: Identify and explain the main themes or topics discussed in the text.\u001b[39m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;124m                2. **Impacts**: Assess the broader impacts, highlighting economic, technological, political, and social dimensions.\u001b[39m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;124m                3. **Examples and Evidence**: Draw connections to real-world examples or supporting evidence that underline the key points.\u001b[39m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;124m                4. **Opportunities and Risks**: Explore potential opportunities and risks suggested by the text.\u001b[39m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;124m                5. **Conclusion**: Summarize the implications and suggest future considerations or actions that align with the insights presented in the text.\u001b[39m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;124m                ---\u001b[39m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;124m            \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchunk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m     86\u001b[0m         }]\n\u001b[0;32m     87\u001b[0m     )\n\u001b[0;32m     88\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m extract_summary(response)\n",
      "File \u001b[1;32mD:\\anaconda3\\envs\\od_zera_do_ai\\Lib\\site-packages\\ollama\\_client.py:236\u001b[0m, in \u001b[0;36mClient.chat\u001b[1;34m(self, model, messages, tools, stream, format, options, keep_alive)\u001b[0m\n\u001b[0;32m    233\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m images \u001b[38;5;241m:=\u001b[39m message\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m    234\u001b[0m     message[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m [_encode_image(image) \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m images]\n\u001b[1;32m--> 236\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request_stream(\n\u001b[0;32m    237\u001b[0m   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPOST\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    238\u001b[0m   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/api/chat\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    239\u001b[0m   json\u001b[38;5;241m=\u001b[39m{\n\u001b[0;32m    240\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m: model,\n\u001b[0;32m    241\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m'\u001b[39m: messages,\n\u001b[0;32m    242\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtools\u001b[39m\u001b[38;5;124m'\u001b[39m: tools \u001b[38;5;129;01mor\u001b[39;00m [],\n\u001b[0;32m    243\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m'\u001b[39m: stream,\n\u001b[0;32m    244\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mformat\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mformat\u001b[39m,\n\u001b[0;32m    245\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moptions\u001b[39m\u001b[38;5;124m'\u001b[39m: options \u001b[38;5;129;01mor\u001b[39;00m {},\n\u001b[0;32m    246\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkeep_alive\u001b[39m\u001b[38;5;124m'\u001b[39m: keep_alive,\n\u001b[0;32m    247\u001b[0m   },\n\u001b[0;32m    248\u001b[0m   stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[0;32m    249\u001b[0m )\n",
      "File \u001b[1;32mD:\\anaconda3\\envs\\od_zera_do_ai\\Lib\\site-packages\\ollama\\_client.py:99\u001b[0m, in \u001b[0;36mClient._request_stream\u001b[1;34m(self, stream, *args, **kwargs)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_request_stream\u001b[39m(\n\u001b[0;32m     94\u001b[0m   \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     95\u001b[0m   \u001b[38;5;241m*\u001b[39margs,\n\u001b[0;32m     96\u001b[0m   stream: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     97\u001b[0m   \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m     98\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Mapping[\u001b[38;5;28mstr\u001b[39m, Any], Iterator[Mapping[\u001b[38;5;28mstr\u001b[39m, Any]]]:\n\u001b[1;32m---> 99\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stream(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\u001b[38;5;241m.\u001b[39mjson()\n",
      "File \u001b[1;32mD:\\anaconda3\\envs\\od_zera_do_ai\\Lib\\site-packages\\ollama\\_client.py:70\u001b[0m, in \u001b[0;36mClient._request\u001b[1;34m(self, method, url, **kwargs)\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_request\u001b[39m(\u001b[38;5;28mself\u001b[39m, method: \u001b[38;5;28mstr\u001b[39m, url: \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m httpx\u001b[38;5;241m.\u001b[39mResponse:\n\u001b[1;32m---> 70\u001b[0m   response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39mrequest(method, url, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     72\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     73\u001b[0m     response\u001b[38;5;241m.\u001b[39mraise_for_status()\n",
      "File \u001b[1;32mD:\\anaconda3\\envs\\od_zera_do_ai\\Lib\\site-packages\\httpx\\_client.py:837\u001b[0m, in \u001b[0;36mClient.request\u001b[1;34m(self, method, url, content, data, files, json, params, headers, cookies, auth, follow_redirects, timeout, extensions)\u001b[0m\n\u001b[0;32m    822\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m)\n\u001b[0;32m    824\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuild_request(\n\u001b[0;32m    825\u001b[0m     method\u001b[38;5;241m=\u001b[39mmethod,\n\u001b[0;32m    826\u001b[0m     url\u001b[38;5;241m=\u001b[39murl,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    835\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mextensions,\n\u001b[0;32m    836\u001b[0m )\n\u001b[1;32m--> 837\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(request, auth\u001b[38;5;241m=\u001b[39mauth, follow_redirects\u001b[38;5;241m=\u001b[39mfollow_redirects)\n",
      "File \u001b[1;32mD:\\anaconda3\\envs\\od_zera_do_ai\\Lib\\site-packages\\httpx\\_client.py:926\u001b[0m, in \u001b[0;36mClient.send\u001b[1;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[0;32m    922\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_timeout(request)\n\u001b[0;32m    924\u001b[0m auth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_request_auth(request, auth)\n\u001b[1;32m--> 926\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_handling_auth(\n\u001b[0;32m    927\u001b[0m     request,\n\u001b[0;32m    928\u001b[0m     auth\u001b[38;5;241m=\u001b[39mauth,\n\u001b[0;32m    929\u001b[0m     follow_redirects\u001b[38;5;241m=\u001b[39mfollow_redirects,\n\u001b[0;32m    930\u001b[0m     history\u001b[38;5;241m=\u001b[39m[],\n\u001b[0;32m    931\u001b[0m )\n\u001b[0;32m    932\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    933\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "File \u001b[1;32mD:\\anaconda3\\envs\\od_zera_do_ai\\Lib\\site-packages\\httpx\\_client.py:954\u001b[0m, in \u001b[0;36mClient._send_handling_auth\u001b[1;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[0;32m    951\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[0;32m    953\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 954\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_handling_redirects(\n\u001b[0;32m    955\u001b[0m         request,\n\u001b[0;32m    956\u001b[0m         follow_redirects\u001b[38;5;241m=\u001b[39mfollow_redirects,\n\u001b[0;32m    957\u001b[0m         history\u001b[38;5;241m=\u001b[39mhistory,\n\u001b[0;32m    958\u001b[0m     )\n\u001b[0;32m    959\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    960\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mD:\\anaconda3\\envs\\od_zera_do_ai\\Lib\\site-packages\\httpx\\_client.py:991\u001b[0m, in \u001b[0;36mClient._send_handling_redirects\u001b[1;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[0;32m    988\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequest\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m    989\u001b[0m     hook(request)\n\u001b[1;32m--> 991\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_single_request(request)\n\u001b[0;32m    992\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    993\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[1;32mD:\\anaconda3\\envs\\od_zera_do_ai\\Lib\\site-packages\\httpx\\_client.py:1027\u001b[0m, in \u001b[0;36mClient._send_single_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   1023\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1024\u001b[0m     )\n\u001b[0;32m   1026\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39mrequest):\n\u001b[1;32m-> 1027\u001b[0m     response \u001b[38;5;241m=\u001b[39m transport\u001b[38;5;241m.\u001b[39mhandle_request(request)\n\u001b[0;32m   1029\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, SyncByteStream)\n\u001b[0;32m   1031\u001b[0m response\u001b[38;5;241m.\u001b[39mrequest \u001b[38;5;241m=\u001b[39m request\n",
      "File \u001b[1;32mD:\\anaconda3\\envs\\od_zera_do_ai\\Lib\\site-packages\\httpx\\_transports\\default.py:236\u001b[0m, in \u001b[0;36mHTTPTransport.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    223\u001b[0m req \u001b[38;5;241m=\u001b[39m httpcore\u001b[38;5;241m.\u001b[39mRequest(\n\u001b[0;32m    224\u001b[0m     method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[0;32m    225\u001b[0m     url\u001b[38;5;241m=\u001b[39mhttpcore\u001b[38;5;241m.\u001b[39mURL(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    233\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[0;32m    234\u001b[0m )\n\u001b[0;32m    235\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[1;32m--> 236\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pool\u001b[38;5;241m.\u001b[39mhandle_request(req)\n\u001b[0;32m    238\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m    240\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[0;32m    241\u001b[0m     status_code\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mstatus,\n\u001b[0;32m    242\u001b[0m     headers\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[0;32m    243\u001b[0m     stream\u001b[38;5;241m=\u001b[39mResponseStream(resp\u001b[38;5;241m.\u001b[39mstream),\n\u001b[0;32m    244\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[0;32m    245\u001b[0m )\n",
      "File \u001b[1;32mD:\\anaconda3\\envs\\od_zera_do_ai\\Lib\\site-packages\\httpcore\\_sync\\connection_pool.py:216\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    213\u001b[0m         closing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign_requests_to_connections()\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_connections(closing)\n\u001b[1;32m--> 216\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    218\u001b[0m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[0;32m    219\u001b[0m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[0;32m    220\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, Iterable)\n",
      "File \u001b[1;32mD:\\anaconda3\\envs\\od_zera_do_ai\\Lib\\site-packages\\httpcore\\_sync\\connection_pool.py:196\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    192\u001b[0m connection \u001b[38;5;241m=\u001b[39m pool_request\u001b[38;5;241m.\u001b[39mwait_for_connection(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[0;32m    194\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    195\u001b[0m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[1;32m--> 196\u001b[0m     response \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39mhandle_request(\n\u001b[0;32m    197\u001b[0m         pool_request\u001b[38;5;241m.\u001b[39mrequest\n\u001b[0;32m    198\u001b[0m     )\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[0;32m    200\u001b[0m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[0;32m    201\u001b[0m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[0;32m    202\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m    203\u001b[0m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[0;32m    204\u001b[0m     pool_request\u001b[38;5;241m.\u001b[39mclear_connection()\n",
      "File \u001b[1;32mD:\\anaconda3\\envs\\od_zera_do_ai\\Lib\\site-packages\\httpcore\\_sync\\connection.py:101\u001b[0m, in \u001b[0;36mHTTPConnection.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m     98\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connect_failed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     99\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[1;32m--> 101\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection\u001b[38;5;241m.\u001b[39mhandle_request(request)\n",
      "File \u001b[1;32mD:\\anaconda3\\envs\\od_zera_do_ai\\Lib\\site-packages\\httpcore\\_sync\\http11.py:143\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    141\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_closed\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[0;32m    142\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response_closed()\n\u001b[1;32m--> 143\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "File \u001b[1;32mD:\\anaconda3\\envs\\od_zera_do_ai\\Lib\\site-packages\\httpcore\\_sync\\http11.py:113\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    102\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[0;32m    105\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreceive_response_headers\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request, kwargs\n\u001b[0;32m    106\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[0;32m    107\u001b[0m     (\n\u001b[0;32m    108\u001b[0m         http_version,\n\u001b[0;32m    109\u001b[0m         status,\n\u001b[0;32m    110\u001b[0m         reason_phrase,\n\u001b[0;32m    111\u001b[0m         headers,\n\u001b[0;32m    112\u001b[0m         trailing_data,\n\u001b[1;32m--> 113\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_receive_response_headers(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    114\u001b[0m     trace\u001b[38;5;241m.\u001b[39mreturn_value \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    115\u001b[0m         http_version,\n\u001b[0;32m    116\u001b[0m         status,\n\u001b[0;32m    117\u001b[0m         reason_phrase,\n\u001b[0;32m    118\u001b[0m         headers,\n\u001b[0;32m    119\u001b[0m     )\n\u001b[0;32m    121\u001b[0m network_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_network_stream\n",
      "File \u001b[1;32mD:\\anaconda3\\envs\\od_zera_do_ai\\Lib\\site-packages\\httpcore\\_sync\\http11.py:186\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_response_headers\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    183\u001b[0m timeout \u001b[38;5;241m=\u001b[39m timeouts\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 186\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_receive_event(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11\u001b[38;5;241m.\u001b[39mResponse):\n\u001b[0;32m    188\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mD:\\anaconda3\\envs\\od_zera_do_ai\\Lib\\site-packages\\httpcore\\_sync\\http11.py:224\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_event\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    221\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mnext_event()\n\u001b[0;32m    223\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11\u001b[38;5;241m.\u001b[39mNEED_DATA:\n\u001b[1;32m--> 224\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_network_stream\u001b[38;5;241m.\u001b[39mread(\n\u001b[0;32m    225\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mREAD_NUM_BYTES, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[0;32m    226\u001b[0m     )\n\u001b[0;32m    228\u001b[0m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[0;32m    229\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m    230\u001b[0m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    234\u001b[0m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[0;32m    235\u001b[0m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[0;32m    236\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;241m==\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mtheir_state \u001b[38;5;241m==\u001b[39m h11\u001b[38;5;241m.\u001b[39mSEND_RESPONSE:\n",
      "File \u001b[1;32mD:\\anaconda3\\envs\\od_zera_do_ai\\Lib\\site-packages\\httpcore\\_backends\\sync.py:126\u001b[0m, in \u001b[0;36mSyncStream.read\u001b[1;34m(self, max_bytes, timeout)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[0;32m    125\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39msettimeout(timeout)\n\u001b[1;32m--> 126\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39mrecv(max_bytes)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "def main():\n",
    "    \"\"\"Main function to run the program.\"\"\"\n",
    "    models = [        \n",
    "        'mistral:7b',\n",
    "        'mistral-small',\n",
    "        'llama3.2:3b',\n",
    "        'qwen2:7b',\n",
    "        'yi',\n",
    "        'glm4:9b',\n",
    "        'qwen2.5:7b',\n",
    "    ]\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-distilroberta-v1\")\n",
    "    embed_model = AutoModel.from_pretrained(\"sentence-transformers/all-distilroberta-v1\")\n",
    "    file_paths = load_file_paths('voiceapp\\\\lista.txt')\n",
    "\n",
    "    for model_name in models:\n",
    "        process_files_for_model(model_name, file_paths, tokenizer, embed_model)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a11d33d-bf4e-432d-a5a4-986fc62e3e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1 Prompt version\n",
    "def generate_summaries_for_chunk(chunk, model_name, tokenizer, embed_model, output_folder):\n",
    "    \"\"\"Generate summary for a single chunk of text.\"\"\"\n",
    "    response = ollama.chat(\n",
    "        model=model_name,\n",
    "        messages=[{\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": f\"\"\"\n",
    "                ---\n",
    "                **\"You are a copywriter. Your task is to summarize the provided text according to the following framework:**\n",
    "                1. **Key Themes**: Identify and explain the main themes or topics discussed in the text.\n",
    "                2. **Impacts**: Assess the broader impacts, highlighting economic, technological, political, and social dimensions.\n",
    "                3. **Examples and Evidence**: Draw connections to real-world examples or supporting evidence that underline the key points.\n",
    "                4. **Opportunities and Risks**: Explore potential opportunities and risks suggested by the text.\n",
    "                5. **Conclusion**: Summarize the implications and suggest future considerations or actions that align with the insights presented in the text.\n",
    "                ---\n",
    "            \"\"\",\n",
    "            \"role\": \"user\",\n",
    "            \"content\": chunk,\n",
    "        }]\n",
    "    )\n",
    "    return extract_summary(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8de6ae-dec2-4ddb-a9e7-8b8b1090ddc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summaries_for_chunk(chunk, model_name, tokenizer, embed_model, output_folder):\n",
    "    \"\"\"Generate summary for a single chunk of text.\"\"\"\n",
    "    response = ollama.chat(\n",
    "        model=model_name,\n",
    "        messages=[{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"\"\"\n",
    "                ---\n",
    "                **\"You are a copywriter. Your task is to summarize the provided text according to the following framework:**\n",
    "                1. **Key Themes**: Identify and explain the main themes or topics discussed in the text.\n",
    "                2. **Impacts**: Assess the broader impacts, highlighting economic, technological, political, and social dimensions.\n",
    "                3. **Examples and Evidence**: Draw connections to real-world examples or supporting evidence that underline the key points.\n",
    "                4. **Opportunities and Risks**: Explore potential opportunities and risks suggested by the text.\n",
    "                5. **Conclusion**: Summarize the implications and suggest future considerations or actions that align with the insights presented in the text.\n",
    "                ---\n",
    "            \"{chunk}\" \"\"\"\n",
    "        }]\n",
    "    )\n",
    "    return extract_summary(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e83f01-874f-44c8-a689-c36cc844ad82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summaries_for_chunk(chunk, model_name, tokenizer, embed_model, output_folder):\n",
    "    \"\"\"Generate summary for a single chunk of text.\"\"\"\n",
    "    response = ollama.chat(\n",
    "        model=model_name,\n",
    "        messages=[{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"\"\"\n",
    "                Imagine you are a university professor specializing in social sciences, particularly in the field of technology and its societal impacts. You are tasked with summarizing an academic article about the influence of emerging technologies, such as artificial intelligence (AI) and automation, on global economies and workforces. The article includes case studies, statistical data, and theoretical frameworks.\n",
    "\n",
    "                Please provide a detailed and well-structured summary of the article, focusing on the following elements:\n",
    "                \n",
    "                1. **Introduction**: Explain the topic of the article, including the emerging technologies being discussed and their broader societal and economic implications. Provide context on why this topic is significant.\n",
    "                2. **Main arguments**: Describe the key discussions or theories presented in the article. Include insights into the challenges and opportunities these technologies create, supported by relevant examples or case studies discussed in the text.\n",
    "                3. **Findings and conclusions**: Outline the main conclusions of the article, including any recommendations, policy suggestions, or future considerations highlighted by the authors.\n",
    "                \n",
    "                Avoid including specific data points, figures, or direct quotes, but ensure that the summary captures the depth and breadth of the article's content. Maintain a formal and academic tone, ensuring the summary is clear, coherent, and suitable for professional use.\n",
    "                \n",
    "                ---\n",
    "                \n",
    "                **Example of the expected summary:**\n",
    "                \n",
    "                1. **Introduction**: This article examines the societal and economic impacts of emerging technologies, particularly artificial intelligence (AI) and automation, focusing on their transformative effects on industries and labor markets worldwide. It highlights the growing significance of these technologies in reshaping global economies and emphasizes the urgency of understanding their implications for workforce adaptation.  \n",
    "                2. **Main arguments**: The article explores how automation and AI have the potential to displace traditional jobs while simultaneously creating new roles in tech-related fields. It provides case studies from countries like the United States, China, and Germany, showcasing varying strategies for adaptation. Challenges such as skill gaps and economic inequality are discussed alongside opportunities for innovation and efficiency.  \n",
    "                3. **Findings and conclusions**: The authors conclude that governments and industries must collaborate to address the challenges posed by technological advancements. Policy recommendations include investing in workforce retraining programs, emphasizing STEM education, and creating safety nets for displaced workers. The article underscores the need for proactive measures to ensure that technological progress leads to inclusive economic growth.  \n",
    "\n",
    "            \"{chunk}\" \"\"\"\n",
    "        }]\n",
    "    )\n",
    "    return extract_summary(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my-conda-env-kernel",
   "language": "python",
   "name": "my-conda-env-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
